\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Processing}
\author{Tom Wasow}

\begin{document}
\maketitle

%\begin{abstract}
%Your abstract.
%\end{abstract}

\section{Introduction}

Little psycholinguistic research has been guided by ideas from HPSG (but see  Konieczny (1996) for a notable exception).  This is not so much a reflection on HPSG as on the state of current knowledge of the relationship between language structure and the unconscious processes that underlie language production and comprehension.  Other theories of grammar have likewise not figured prominently in theories of language processing, at least in recent decades\footnote{Half a century ago, the Derivational Theory of Complexity (DTC) was an attempt to use psycholinguistic experiments to test aspects of the grammatical theory that was dominant at the time.  The DTC was discredited in the 1970s, and the theory it purported to support has long-since been superceded.  See Fodor, Bever, and Garrett (1974) for discussion.}  The focus of this chapter, then, will be on how well the architecture of HPSG comports with available evidence about language production and comprehension.  

My argument is much the same as that put forward by Sag, Wasow, \& Bender (2003; Chapter 9), and Sag \& Wasow (2011, 2015), but with some additional observations about the relationship between competence and performance. I presuppose the ``competence hypothesis'' (see Chomsky, 1965, Chapter 1), that is, that a theory of language use (performance) should incorporate a grammar representing the knowledge of language (competence) that is drawn on in everyday comprehension and production, as well as in other linguistic activities, such as language games and the (often artificial) tasks employed in psycholinguistic experiments.  

The primary reason for adopting the competence hypothesis is parsimony:  a theory of language use is simpler if it does not have to repeat much the same information about about the language in both its production and comprehension components.  Such information would include such things as the vocabulary, the preferred word orders, and most of the rest of what linguists encode in their grammars.  A performance theory that incorporates a grammar only needs to include such information once.\footnote{There are of course some discrepancies between production and comprehension that need to be accounted for in a full theory of language use.  For example, most people can understand some expressions that they never use, including such things as dialect-specific words or accents.  But these discrepancies are on the margins of speakers' knowledge of their languages.  The vast majority of the words and structures that speakers know are used in both production and comprehension.  Further, it seems to be generally true that what speakers can produce is a proper subset of what they can comprehend.  Hence, the discrepancies can plausibly be attributed to performance factors such as memory or motor habits} 

There is also, however, an empirical reason for preferring a model with a good fit between competence and performance.  As noted by Bresnan, Dingare, \& Manning (2001), preferences that are only statistical tendencies in some languages can show up in others as categorical requirements.  The example they discuss in detail is the avoidance of clauses with third-person subjects but first- or second-person objects or obliques. In English, this is a powerful statistical tendency, which they document by showing that the passivization rate in the Switchboard corpus is very significantly lower when the agent is first- or second-person than when it is third-person.  In Lummi (a Salish language of British Columbia), this preference is categorical:  clauses with third-person subjects but first- or second-person objects or obliques are simply unacceptable.  Hawkins (2004, 2014) argues that such examples are by no means exceptional, and formulates the following ``Performance-Grammar Correspondence Hypothesis'' (PGCH):
\begin{quote}
Grammars  have  conventionalized  syntactic  structures  in  proportion   to their degree of preference in performance, as evidenced by frequency of use and ease of processing.\footnote {In the Bresnan, et al example, I know of no experimental evidence that clauses with third-person subjects and first- or second-person objects are difficult to process.  But a plausible case can be made that the high salience of speaker and addressee makes the pronouns referring to them more accessible in both production and comprehension than expressions referring to other entities.  In any event, clauses with first- or second-person subjects and third -person objects are far more frequent than clauses with the reverse pattern in languages where this has been checked.  Thus, the Bresnan, et al example falls under the PGCH, at least with respect to ``frequency of use'' .}
\end{quote}
There are two ways in which a processing model incorporating a grammar might capture this generalization.  One is to give up the widespread assumption that grammars provide categorical descriptions, and that any quantitative generalizations must be extra-grammatical; see Francis (in preparation) for arguments supporting this option, and thoughful discussion of literature on how to differentiate processing effects from grammar.  For example, some HPSG feature structures might allow multiple values for the same feature, but with probabilities (adding up to 1) attached to each value.\footnote{I discussed this idea many times with the late Ivan Sag.  He made it clear that he believed grammatical generalizations should be categorical.  In part for that reason, this idea was not included in our joint publications on processing and HPSG}  I hasten to add that fleshing out this idea into a full-fledged probabilistic version of HPSG would be a large undertaking, well beyond the scope of this chapter; see Linadarki (2006), and Miyao and Tsujii (2008) for work along these lines.  But the idea is fairly straightforward, and would allow, for example, English to have {\bf in its grammar} a non-categorical constraint against clauses with third-person subjects and first- or second-person objects or obliques.  

The second way for a theory adopting the competence hypothesis to represent Hawkins's PGCH would be to allow certain generalizations to be stated either as grammatical constraints (when they are categorical) or as probabilistic performance constraints.  This requires a fit between the grammar and the other components of the performance model that is close enough to permit what is essentially the same generalization to be expressed in the grammar or elsewhere.  In the case discussed by Bresnan, et al, for example, treating the constraint in question as part of the grammar of Lummi but a matter of performance in English would require that both the theory of grammar and models of production would include, minimally, the distinction between third-person and other persons, and the distinction between subjects and non-subjects.  Since virtually all theories of grammar make these distinctions, this observation is not very useful in choosing among theories of grammar.  I will return later to phenomena that bear on the choice among grammatical theories, at least if one accepts the competence hypothesis.

Since its earliest days, HPSG research has been motivated in part by considerations of computational tractability (see Flickinger, Pollard, \& Wasow, this volume, for discussion).  Some of the design features of the theory can be traced back to the need to build a system that could run on the computers of the 1980s.  Despite the obvious differences between human and machine information processing, some aspects of HPSG's architecture that were initially motivated on computational grounds have turned out to fit well with what is known about human language processing.  A prime example of that is the computational analogue to the competence hypothesis, namely, the fact that the same grammar is used for parsing and generation. In section 3, I will discuss a number of other high-level design properties of HPSG, arguing that they fit well with what is known about human language processing.  In section 4, I will briefly discuss two phenomena that have been the locus of much discussion about the relationship between grammar and processing, namely, island constraints and differences between subject and object relative clauses.

\section{Key Facts about Human Language Processing}

In this section I review a number of well-known general properties of human language processing.  Most of them seem evident from subjective experience of language use, but there is supporting experimental evidence for all of them.  

\subsection{Incrementality}

Both language production and comprehension proceed incrementally, from the beginning to the end of an utterance.  In the case of production, this is evident from the fact that utterances unfold over time.  Moreover, speakers typically begin their utterances without having fully planned them out, as is evident from the prevalence of disfluencies.  On the comprehension side, there is considerable evidence that listeners (and readers) begin analyzing input right away, without waiting for utterances to be complete.  A grammatical theory that assigns structure and meaning to initial substrings of sentences will fit more naturally than one that doesn't into a processing model that exhibits this incrementality we see in human language use.  

I hasten to add that there is also good evidence that both production and comprehension involve anticipation of later parts of sentences.  While speakers may not have their sentences fully planned before they begin speaking, some planning of downstream words must take place.  This is perhaps most evident from instances of nouns exhibiting quirky cases determined by verbs that occur later in the clause.  For example, objects of German {\it helfen}, `help', take the dative case, rather than the default accusative for direct objects.  But in a sentence like (1), the speaker must know that the verb will be one taking a dative object at the time {\it dem} is uttered.
\ea
\gll Wir werden dem        Kind bald helfen.\\
     we will    the.\dat{} child soon help\\\german
\glt `We will help the child soon.'
\end{enumerate}
Likewise, in comprehension there is ample evidence that listeners and readers anticipate what is to come.  This has been demonstrated using a variety experimental paradigms.  Eye-tracking studies (see Tannenhaus, et al 1995, Altmann \& Kamide 1999, Arnold, et al 2007, among many others) have shown that listeners use semantic information and world knowledge to predict what speakers will refer to next.

Thus, a theory of grammar that fits comfortably into a model of language use should provide representations of initial substrings of utterances that can be assigned (partial) meanings and be used in predicting later parts of those utterances.

\subsection{Non-modularity}

Psycholinguistic research over the past four decades has established that language processing involves integrating a wide range of types of information on an as-needed basis.  That is, the various components of the language faculty interact throughout their operation.  A model of language use should therefore {\bf not} be modular, in the sense of Jerry Fodor's influential (1983) book, {\it The Modularity of Mind}.\footnote {Much of the psycholinguistic research of the 1980s was devoted to exploring modularity -- that is, the idea that the human linguistic faculty consists of a number of distinct ``informationally encapsulated'' modules.  While Fodor's book was mostly devoted to arguing for modularity at a higher level, where the linguistic faculty was one module, many researchers at the time extended the idea to the internal organization of the linguistic faculty, positing largely autonomous mechanisms for phonology, syntax, semantics, and pragmatics, with the operations of each of these sub-modules unaffected by the operations of the others. The outcome of years of experimental studies on the linguistic modularity idea was that it was abandoned by most psycholinguists. For an early direct response to Fodor, see Marslen-Wilson and Tyler (1987)}
\newline
\newline
Some casual observations argue against modular language processing.  For example, the famously ambiguous sentences (2) and (3) are disambiguated in speech by the stress patterns.
\begin{quote}
2. I forgot how good beer tastes.
\newline3. Dogs must be carried.
\end{quote}
The two meanings of (2) correspond to two different parses (one with {\it good} as part of the noun phrase {\it good beer} and the other with {\it how good} as a verb phrase modifier).  The two meanings of (3) have the same syntactic structure, but differ in whether the requirement is that all dogs be carried, or that everyone carry a dog.  This interaction of prosody with syntax (in the case of (2)) and with semantics (in the case of (3)) is produced and perceived before the end of the utterance, suggesting that phonological information is available in the course of syntactic and semantic processing.  
\newline
\newline
Moreover, non-linguistic knowledge influences the disambiguation in both of these cases.  If (2) is preceded by ``I just finished three weeks without alcohol,'' the natural interpretation of {\it good} is as a modifier of {\it tastes}; but following ``I just finished three weeks drinking only Bud Light,'' {\it good} is more naturally interpreted as a modifier of {\it beer}.  In the case of (3), only one interpretation (that anyone with a dog must carry it) is plausible, given our knowledge of the world.  Indeed, most non-linguists fail to see the ambiguity of (3) without a lengthy explanation.  
\newline
\newline
More rigorous evidence of the non-modular character of language processing has been provided by a variety of types of experiments.  The work of Michael Tanenhaus and his associates, using eye-tracking to investigate the time-course of sentence comprehension, played an important role in convincing most psycholinguists that human language understanding is non-modular.  See, for example, Eberhart, et al (1995), McMurray, et al (2008), Tanenhaus, et al (1995), Tanenhaus, et al (1996), and Tanenhaus and Trueswell (1995). A recent survey of work arguing against modularity in language processing is provided by Spevack, et al (2018).  

\subsection{Importance of Words}

The individual properties of words play a central role in how people process phrases and sentences.  Consider, for example, what is probably the most famous sentence in psycholinguistics, (4).
\begin{quote}
4. The horse raced past the barn fell.
\end{quote}
The extreme difficulty that people who have not previously been exposed to (4) have comprehending it depends heavily on the choice of words.  A sentence like (5), with the same syntactic structure, is far easier to parse.
\begin{quote}
5. The applicant interviewed in the morning left.
\end{quote}  
Numerous studies (e.g. Trueswell, et al (1993), MacDonald, et al (1994), Bresnan, et al (2007), Wasow, et al (2011))  have shown that such properties of individual words as subcategorization preferences, semantic categories (e.g. animacy), and frequency of use can influence the processing of utterances.  

\subsection{Influence of Context}

Much of the evidence against modularity of the language faculty is based on the influences of non-linguistic context and world knowledge on language processing.  The well-known McGurk effect (McGurk and MacDonald (1976)) and the Stroop effect (Stroop (1935)) demonstrate that, even at the word level, visual context can influence linguistic comprehension and production.
\newline
\newline
Linguistic context also clearly influences processing, as the discussion of examples (2) and (3) above illustrates.  The same conclusion is supported by numerous controlled studies, including, among many others, those described by Crain and Steedman (1985), Altmann and Steedman (1988), Branigan (2007), Traxler and Tooley (2007),and Spevack, et al (2018).  The last of these references concludes, ``when humans and their brains are processing language with each other, there is no format
of linguistic information (e.g., lexical, syntactic, semantic, and pragmatic) that cannot be rapidly
influenced by context.''

\subsection{Speed and Accuracy of Processing}

A good deal of psycholinguistic literature is devoted to exploring situations in which language processing encounters difficulties, notably work on garden paths (in comprehension) and disfluencies (in production).  Much more striking than the existence of these phenomena, however, is how little they matter in everyday language use.  While ambiguities abound in normal sentences (see Wasow (2015)), comprehenders very rarely experience noticeable garden paths.  Similarly, disfluencies in spontaneous speech occur in nearly every sentence but rarely disrupt communication.  
\newline
\newline
People are able to use speech to exchange information remarkably efficiently.  A successful account of human language processing must explain why it works as well as it does.  

\section{Features of HPSG that Fit Well with Processing Facts}

In this section, I review some basic design features of HPSG, pointing out ways in which they comport well with the properties of language processing listed in the previous section.

\subsection{Constraint-based}

Well-formedness of HPSG representations is defined by the simultaneous satisfaction of a set of constraints that constitutes the grammar.  This lack of directionality allows the same grammar to be used in modeling production and comprehension.
\newline
\newline
Consider, for instance, the example of quirky case assignment illustrated in (1) above.  A speaker uttering (1) would need to have planned to use the verb {\it helfen} before beginning to utter the object NP.  But a listener hearing (1) would encounter the dative case on the article {\it dem} before hearing the verb and could infer only that a verb taking a dative object was likely to occur at the end of the clause.  Hence, the partial mental representations built up by the two interlocutors during the course of the utterance would be quite different.  But the grammatical mechanism licensing the combination of a dative object with this particular verb is the same for speaker and hearer. 
\newline
\newline
In contrast, theories of grammar that utilize sequential operations to derive sentences impose a directionality on their grammars.  If such a grammar is then to be employed as a component in a model of language use (as the competence hypothesis stipulates), its inherent directionality becomes part of the models of both production and comprehension.  But production involves mapping meaning onto sound, whereas comprehension involves the reverse mapping.  Hence, a directional grammar cannot fit the direction of processing for both production and comprehension.\footnote{This was an issue for early work in computational linguistics that built parsers based on the transformational grammars of the time, which generated sentences using derivations whose direction went from an underlying structure largely motivated by semantic considerations to the observable surface structure.  See, for example, Hobbs and Grishman (1975).}  
\newline
\newline
Branigan and Pickering (2017) argue at length that ``structural priming provides an implicit method of investigating linguistic representations.''  They go on to conclude that the evidence from priming supports ``frameworks that ... assume nondirectional and constraint-based generative
capacities (i.e., specifying well-formed structures) that
do not involve movement.''  HPSG is one of the frameworks they mention that fit this description.

\subsection{Surface-oriented}

The features and values in HPSG representations are motivated
by straightforwardly observable linguistic phenomena. HPSG does not posit derivations of observable properties from abstract underlying structures.  In this sense it is surface-oriented.
\newline
\newline
The evidence linguists use in formulating grammars consists of certain types of performance data, primarily judgments of acceptability and meaning.  Accounts of the data necessarily involve some combination of grammatical and processing mechanisms.  The closer the grammatical descriptions are to the observable phenomena, the less complex the processing component of the account needs to be.
\newline
\newline
For example, the grammatical theory of Kayne (1994), which posits a universal underlying order of specifier-head-complement, requires elaborate (and directional) transformational derivations to relate these underlying structures to the observable data in languages whose surface order is different (a majority of the language of the world).  In the absence of experimental evidence that the production and comprehension of sentences with different constituent orders involves mental operations corresponding to the grammatical derivations Kayne posits, his theory of grammar seems to be incompatible with the competence hypothesis.
\newline
\newline
Experimental evidence supports this reasoning.  As Branigan and Pickering (2017) conclude, ``[P]riming evidence supports the existence of
abstract syntactic representations. It also suggests that
these are shallow and monostratal in a way that corresponds
at least roughly to the assumptions of ... Pollard and
Sag (1994)... It does not support a second,
underlying level of syntactic structure or the syntactic representation
of empty categories associated with the movement
of constituents in some transformational analyses.''
\subsection{Informationally Rich Representations}

The feature structures of HPSG include all
types of linguistic information relevant to the well-formedness and interpretation
of expressions. This includes phonological, morphological, syntactic, semantic, and contextual information.  They can also incorporate non-linguistic contextual information (e.g. social information), though this has not been extensively explored.
\newline
\newline
The cooccurrence of these different types of information within a single representation facilitates modeling production and comprehension processes that make reference to more than one of them.  The architecture of the grammar is thus well suited to the non-modularity and context-sensitivity of language processing.  
\newline
\newline
It is interesting in this regard to consider the conclusions of two papers by psycholinguists who surveyed experimental evidence and inferred what types of grammatical information was essential for processing.  
\newline
\newline
The following series of quotes captures the essence of what MacDonald, et al (1994) wrote regarding lexical representation:
\begin{itemize} 
\item ``[T]he lexical
representation for a word includes a representation of the
word's phonological form, orthographic form, semantics,
grammatical features (including grammatical category), morphology
(at least inflectional), argument structure, and X-bar
structure.''
\item ``[T]he connection
structure of the lexicon encodes relationships among
different types of lexical information.''
\item
``In addition to constraints that hold between various aspects
of lexical representations, sentence and discourse contexts also
constrain lexical representations during processing...''
\end{itemize}
With the possible exception of ``X-bar structure'', this sounds very much like a description of the types of information included in HPSG feature structures.
\newline
\newline
Over twenty years later, Branigan and Pickering (2017) came to the following conclusions about linguistic representations:
\begin{itemize}
\item ``The syntactic representations capture local relationships
between a `mother' and its constituent `daughter(s)' (e.g.,
a VP comprising a verb and two NPs), independent of the
larger context in which the phrase appears (e.g., that the VP
occurs within a subordinate clause), or the internal structure
of the subphrases that constitute it (e.g., that the
first NP comprises a determiner, adjective, and noun).''
\item ``[S]ome elements that are not phonologically represented may
be syntactically represented.''
\item ``Other priming evidence similarly indicates that some
semantically specified elements are not specified syntactically.''
\item ``[T]he semantic level of representation
contains at least specifications of quantificational information,
information structure, and thematic roles.''
\item ``Evidence
from priming supports a range of mappings between information encoded in the semantic representation and information encoded in the syntactic representation: between
thematic roles and grammatical functions, between thematic roles and word order, between animacy and syntactic
structure, and between event structures and syntactic
structures.''
\end{itemize}
The two lists are quite different.  This is in part because the focus of the earlier paper was on lexical representations, whereas the later paper was on linguistic representations more generally.  It may also be attributable to the fact that McDonald, et al, framed their paper around the issue of ambiguity resolution, while Branigan and Pickering's paper concentrated on what could be learned from structural priming studies.  Despite these differences, it is striking that the conclusions of both papers about the mental representations employed in language processing are very much like those arrived at by work in HPSG.

\subsection{Lexicalism}

A great deal of the information used in licensing sentences in HPSG is stored in the lexical entries for words.  A hierarchy of lexical types permits commonalities to be factored out to minimize what has to be stipulated in individual entries, but the information in the types gets into the representations of phrases and sentences through the words that instantiate those types. Hence, it is largely the information coming from the words that determines the well-formedness of larger expressions.  Any lexical decomposition would have to be strongly motivated by the morphology.
\newline
\newline
Branigan and Pickering (2017) note that grammatical structures (what some might call ``constructions'') such as V-NP-NP can prime the use of the same abstract structure, even in the absence of lexical overlap.  But they also note that the priming is consistently significantly stronger when the two instances share the same verb.  They call this ``the lexical boost.''  They write, ``To explain abstract
priming, lexicalist theories must assume that the syntactic
representations ...
are shared across lexical entries.'' The types in HPSG's lexicon provide just such representations.  Branigan and Pickering go on to say that the lexical boost argues for ``a representation that
encodes a binding between constituent structure and the
lemma ... of the lexical entry for the
head.''  In HPSG, this ``binding'' is simply the fact that the word providing the lexical boost (say, {\it give}) is an instantiation of a type specifying the structures it appears in (e.g. the ditransitive verb type).
\newline
\newline
Similarly, the fact, noted in section 2.3 above, that a given structure may be more or less difficult to process depending on word choice is unsurprising in HPSG, so long as the processor has access to information about individual words and not just their types.   
\subsection{Underspecification}

HPSG allows a class of linguistic structures that share some feature values to be characterized by means of feature structures that specify only the features whose values are shared.  Such underspecification is very useful for a model of processing (particularly a model of the comprehender) because it allows partial descriptions of the utterance to be built up, based on the information that has been encountered.  This property of the grammar makes it easy to incorporate into an incremental processing model.

\section{Two Phenomena of Interest}
\subsection{Island Constraints}

Ever since Ross's seminal dissertation (1967) introduced the notion of ``island constraints,'' linguists have sought explanations for their existence, often suggesting that they were motivated by processing considerations (notably Grosu (1972), Fodor (1983), Deane (1991)).  The basic idea is that island constraints restrict the search space the parser needs to consider in looking for a gap to match a filler it has encountered, thereby facilitating processing.  This then raises the question of whether island constraints need to be represented in grammar (language particular or universal), or can be attributed entirely to processing and/or other factors, such as pragmatics.
\newline
\newline
In principle, this question is orthogonal to the choice among theories of grammar.  But in recent years, a controversy has arisen between some proponents of HPSG and certain transformational grammarians, with the former (e.g. Chaves (2012 and this volume), Hofmeister and Sag (2010), Hofmeister, et al (2013)) arguing that certain island phenomena should be attributed entirely to extra-grammatical factors, and the latter (e.g. Phillips (2013), Sprouse,et al (2012)) arguing that island constraints are part of grammar.
\newline
\newline
I will not try to settle this dispute here.  Rather, my point in this subsection is to note that a theory in which there is a close fit between the grammar and processing mechanisms allows for the possibility that some island phenomena should be attributed to grammatical constraints, whereas others should be explained in terms of processing.  Indeed, if the basic idea that islands facilitate processing is correct, it is possible that some languages, but not others, have grammaticalized some islands, but not others.  That is, in a theory in which the grammar is a tightly integrated component of a processing model, the question of whether a particular island phenomenon is due to a grammatical constraint is an empirical one, whose answer might differ from language to language. 
\newline
\newline
Early work on islands assumed that, in the absence of negative evidence, island constraints could not be learned and hence must be innate and therefore universal.  But cross-linguistic variation in island constraints, even between closely related languages, has been noted since the early days of research on the topic (see, e.g. Erteschik-Shir (1973) and Engdahl and Ejerhed (1982)).
\newline
\newline
This situation is what one might expect if languages differ with respect to the extent to which the processing factors that motivate islandhood have been grammaticalized.  In short, a theory with a tight fit between its grammatical machinery and its processing mechanisms allows for hybrid accounts of islands that are not available to theories without such a fit.
\newline
\newline
One example of such a hybrid is Chaves's (2012) account of Ross's Coordinate Structure Constraint.  Following much earlier work, he distinguishes between the ``conjunct constraint,'' which prohibits a gap from serving as a conjunct in a coordinate structure (as in *{\it What did you eat a sandwich and?}) and the ``element constraint,'' which prohibits a gap from serving as an element of a larger conjunct (as in *{\it What did you eat a sandwich and a slice of?}).  The conjunct constraint, he argues, follows from the architecture of HPSG and is therefore built into the grammar.  The element constraint, on the other hand, has exceptions and, he claims, should be attributed to extra-grammatical factors.  See Chaves's chapter on islands in this volume for more detailed discussion. 

\subsection{Subject vs. Object Relative Clauses}

One of the most discussed phenomena in the literature on human sentence processing is the difference in processing complexity between relative clauses (RCs) in which the gap is the subject and those in which the gap is the object -- or, as they are commonly called, ``subject RCs'' and ``object RCs''; see, among many others, Wanner and Maratsos (1978), Gibson (1998), Traxler, et al (2002), Gennari and MacDonald (2008).  Relative clause processing complexity has been shown to be influenced by a number of other factors than the grammatical function of the gap, including the animacy and pronominality of the overt NP in the RC\footnote{The stimuli in the experimental studies on this topic always have RCs with one overt NP, either in subject or object position and a gap corresponding to the other grammatical function.}, as well as the frequency, animacy, and discourse properties of the head of the RC.  When these factors are controlled for, however, most psycholinguists accept that it has been established that subject RCs are generally easier to process than object RCs, at least in English.\footnote{This processing difference corresponds to the top end of the ``accessibility hierarchy'' that Keenan and Comrie (1977) proposed as a linguistic universal. Based on a diverse sample of 50 languages, they proposed the hierarchy below, and hypothesized that any language allowing RC gaps at any point in the hierarchy would allow RC gaps at all points higher (to the left) on the hierarchy.
\begin{quote}

Subject 
> 
Direct Object 
> 
Indirect Object 
> 
Oblique 
> 
Genitive
> 
Object of Comparison
\end{quote}
Keenan and Comrie speculated that the generality of this hierarchy of relativizability lay in processing, specifically on the comprehension side.  The extensive experimental evidence that has been adduced in support of this idea in the intervening decades has been concentrated on subject RCs vs. (direct) object RCs.  The remainder of the hierarchy remains largely untested by psycholinguists.}
\newline
\newline
One approach to explaining this asymmetry has been based on the distance between the filler and the gap (see, among others, Wanner and Maratsos (1978), Gibson (1998), Hawkins (2004)).  In languages like English, with basic SVO clause order and RCs that follow the nouns they modify, the distance between the filler (the relativizer or head noun) and the gap is greater for an object gap than for a subject gap.  If holding a filler in memory until the gap is encountered puts an extra burden on the processor, this could explain why object RCs are harder to process than subject RCs.   This distance-based account makes an interesting prediction for languages with different word orders.  In languages like Japanese with SOV order and RCs that precede the nouns they modify, the distance relationships are reversed -- that is, the gaps in object RCs are closer to their fillers than those in subject RCs.  The same is true of Chinese, with basic SVO order and RCs that precede the nouns they modify.  So the prediction of distance-based accounts of the subject/object RC processing asymmetry is that it should be reversed in these languages.
\newline
\newline
The experimental evidence on this prediction is somewhat equivocal.  While Hsiao and Gibson (2003) found a processing preference for object RCs over subject RCs in Chinese, their findings were challenged by Lin and Bever (2006) and Vasishth, et al (2013), who claimed that Chinese has a processing preference for subject RCs.  In Japanese, Miyamoto and Nakamura (2003) found that subject RCs were processed more easily than object RCs.  The issue remains controversial, but, for the most part, the evidence has not supported the idea that the processing preference between subject RCs and object RCs varies across languages with different word orders.
\newline
\newline
The most comprehensive treatment of English RCs in HPSG is Sag (1997). Based entirely on distributional evidence, Sag's analysis treats (finite) subject RCs as fundamentally different from RCs whose gap does not function as the subject of the RC.  The difference is that the SLASH feature, which encodes information about long-distance dependencies in HPSG, plays no role in the analysis of subject RCs.  Non-subject RCs, on the other hand involve a non-empty SLASH value in the RC.\footnote{The idea that at least some subject gaps differ in this fundamental way from non-subject gaps goes back to Gazdar (1981)}   
\newline
\newline
Sag deals with a wide variety of kinds of RCs. From the perspective of the processing literature, the two crucial kinds are exemplified by (6) and (7), from Gibson (1998).
\begin{quote}
6. The reporter who attacked the Senator admitted the error.
\newline
7. The reporter who the Senator attacked admitted the error.
\end{quote}
A well-controlled experiment on the processing complexity of subject and object RCs must have stimuli that are matched in every respect except the role of the gap in the RC.  Thus, the conclusion that object RCs are harder to process than subject RCs is based on a wide variety of studies using stimuli like (6) and (7).  Sag's analysis of (6) posits an empty SLASH value in the RC, whereas his analysis of (7) posits a non-empty SLASH value.  
\newline
\newline
There is considerable experimental evidence supporting the idea that unbounded dependencies -- that is, what HPSG encodes with the SLASH feature -- add to processing complexity; see, for example, Wanner and Maratsos (1978), King and Just (1991), Kluender and Kutas (1993), Hawkins (1999).  Combined with Sag's HPSG analysis of English RCs, this provides an explanation of the processing preference of subject RCs over object RCs.  On such an account, the question of which other languages will exhibit the same preference boils down to the question of which other languages have the same difference in the grammar of subject and object RCs.  At least for English, this is a particularly clear case in which the architecture of HPSG fits well with processing evidence.

\section{Conclusion}

This chapter opened with the observation that HPSG has not served as the theoretical framework for much psycholinguistic research.  The observations in sections 2 through 4 argue for rectifying that situation.  The fit between the architecture of HPSG and what is known about human sentence processing suggests that HPSG could be used to make processing predictions that could be tested in the lab.  
\newline
\newline
To take one example, the explanation of the processing asymmetry between subject and object RCs offered above is based on a grammatical difference in the HPSG analysis:  all else being equal, expressions with non-empty SLASH values are harder to process than those with empty SLASH values.  Psycholinguists could test this idea by looking for other cases of phenomena that look superficially very similar but whose HPSG analyses differ with respect to whether SLASH is empty.  One such case is occurs with pairs like Chomsky's famous minimal pair in (8) and (9).
\begin{quote}
8. Chris is eager to please.
\newline
9. Chris is easy to please.
\end{quote}
Under the analysis of Pollard and Sag (1994), {\it to please} in (9) has a non-empty SLASH value but an empty SLASH value in (8).  Processing (9) should therefore be easier.  This prediction could be tested experimentally, and modern methods such as eye-tracking could pinpoint the locus of any difference in processing complexity to determine whether it corresponds to the region where the grammatical analysis involves a difference in SLASH values.
\newline
\newline
The disconnect between theoretical investigations of language structure and psycholinguistic studies is an unfortunate feature of our discipline.  Because HPSG comports so well with what is known about processing, it could serve as the basis for a reconnection between these two areas of study.
%\section{Some examples to get started}
%
%\subsection{How to add Comments}
%
%Comments can be added to your project by clicking on the comment icon in the toolbar above. % * <john.hammersley@gmail.com> 2014-09-03T09:54:16.211Z:
%
% Here's an example comment!
%

% To reply to a comment, simply click the reply button in the lower right corner of the comment, and you can close them when you're done.

% \subsection{How to include Figures}

% First you have to upload the image file from your computer using the upload link in the project menu. Then use the includegraphics command to include it in your document. Use the figure environment and the caption command to add a number and a caption to your figure. See the code for Figure \ref{fig:frog} in this section for an example.

% \begin{figure}
% \centering
% \includegraphics[width=0.3\textwidth]{frog.jpg}
% \caption{\label{fig:frog}This frog was uploaded via the project menu.}
% \end{figure}

% \subsection{How to add Tables}

% Use the table and tabular commands for basic tables --- see Table~\ref{tab:widgets}, for example. 

% \begin{table}
% \centering
% \begin{tabular}{l|r}
% Item & Quantity \\\hline
% Widgets & 42 \\
% Gadgets & 13
% \end{tabular}
% \caption{\label{tab:widgets}An example table.}
% \end{table}

% \subsection{How to write Mathematics}

% \LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
% \[S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
%       = \frac{1}{n}\sum_{i}^{n} X_i\]
% denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.


% \subsection{How to create Sections and Subsections}

% Use section and subsections to organize your document. Simply use the section and subsection buttons in the toolbar to create them, and we'll handle all the formatting and numbering automatically.

% \subsection{How to add Lists}

% You can make lists with automatic numbering \dots

% \begin{enumerate}
% \item Like this,
% \item and like this.
% \end{enumerate}
% \dots or bullet points \dots
% \begin{itemize}
% \item Like this,
% \item and like this.
% \end{itemize}

% \subsection{How to add Citations and a References List}

% You can upload a \verb|.bib| file containing your BibTeX entries, created with JabRef; or import your \href{https://www.overleaf.com/blog/184}{Mendeley}, CiteULike or Zotero library as a \verb|.bib| file. You can then cite entries from it, like this: \cite{greenwade93}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|.

% You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.

% We hope you find Overleaf useful, and please let us know if you have any feedback using the help menu above --- or use the contact form at \url{https://www.overleaf.com/contact}!

T.M. Altmann, Gerry and Yuki Kamide (1999)
``Incremental interpretation at verbs: restricting the domain of subsequent reference,''
{\it Cognition} 73(3):247-264
\newline
\newline
 Altmann, Gerry  and Mark Steedman (1988) ``Interaction with context during human sentence processing'' 
{\it Cognition} 30(3):191-238
\newline
\newline
Arnold, Jennifer E., Carla Hudson Kam, and Michael K. Tanenhaus (2007). ``If you say thee uh- you’re describing something hard: the on-line attribution of disfluency during reference comprehension'' {\it Journal of Experimental Psychology: Learning, Memory, and Cognition} 33: 914-930.
\newline
\newline
Branigan, Holly (2007) ``Syntactic Priming''.  {\it Language and Linguistics Compass} 1/1–2 (2007): 1–16
\newline
\newline
Branigan, Holly and Martin Pickering (2017) ``An experimental approach to
linguistic representation''.  {\it Behavioral and Brain Sciences} 40: 1-61
\newline
\newline
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald Baayen. 2007. ``Predicting the Dative Alternation'' in  G. Boume, I. Kraemer, and J. Zwarts (eds) {\it Cognitive Foundations of Interpretation}: 69-94  Royal Netherlands Academy of Science
\newline
\newline
Bresnan, Joan, Shipra Dingare, and Christopher D. Manning (2001) ``Soft Constraints Mirror Hard
Constraints: Voice and Person in English and
Lummi'' in M. Butt and T. King (eds) {\it Proceedings of the LFG 01
Conference}
CSLI
Publications
\newline
\newline
Chaves, Rui P. (2012) ``On the Grammar of Extraction and Coordination'' {\it Natural Language and Linguistic Theory}, 30(2): 465–512.
\newline
\newline
Chomsky, Noam (1965) {\it Aspects of the Theory of Syntax}  MIT Press.
\newline
\newline
S. Crain, M.J. Steedman (1985)
``On Not Being Led Up the Garden Path: The Use of Context by the Psychological Parser'' in
D. Dowty, L. Karttunen, A. Zwicky (Eds.) {\it Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives}: 320-358 Cambridge Univ. Press
\newline
\newline
Deane, Paul D. (1991) ``Limits to Attention: A Cognitive Theory
of Island Phenomena''
{\it Cognitive
Linguistics}
2(1): 1–63.
\newline
\newline
Engdahl, Elisabet and Eva Ejerhed (1982) {\it Readings on Unbounded Dependencies in Scandinavian Languages}.  Almqvist and Wiksell International.
\newline
\newline
Erteschik-Shir, Nomi (1973) {\it On the nature of Island Constraints}.  Unpublished MIT dissertation.
\newline
\newline
Fodor, Janet D. (1983) ``Phrase structure parsing and the island constraints''
{\it Linguistics and
Philosophy}
6: 163–223
\newline
\newline
Fodor, Jerry, Thomas Bever, and Merrill Garrett (1974) {\it The Psychology of Language: An Introduction to Psycholinguistics and Generative Grammar}.  McGraw-Hill.
\newline
\newline
Gazdar, Gerald (1981) ``Unbounded Dependencies and Coordinate Structure'' {\it Linguistic Inquiry} 12(2): 155-184.
\newline
\newline
Gennari, Silvia P. and Maryellen C. MacDonald (2008) ``Semantic Indeterminacy in Object Relative Clauses''.  {\it Journal of Memory and Language}.  8: 161–187.
\newline
\newline
Gibson, Edward (1998)
``Linguistic complexity: Locality of syntactic dependencies''
{\it Cognition} 68 (1): 1-76
\newline
\newline
 Grosu, A. (1972) {\it The Strategic Content of Island Constraints}
 unpublished doctoral dissertation, Ohio State University.
\newline
\newline
Eberhard, Kathleen M., Michael J. Spivey-Knowlton, Julie C. Sedivy, and Michael K. Tanenhaus (1995) ``Eye Movements as a Window into Real-time Spoken Language Comprehension in Natural Contexts'' {\it Journal of Psycholinguist Research} 24: 409-436
\newline
\newline
Francis, Elaine (in preparation) {\it Gradient Acceptability and Linguistic Theory}
\newline
\newline
Hawkins John (1999) ``Processing Complexity and Filler-Gap Dependencies across Grammars'' {\it Language} 75:  244–285.
\newline
\newline
Hawkins, John (2044) {\it Efficiency and Complexity in Grammars}  Oxford University Press.
\newline
\newline
Hawkins, John (2014) {\it Cross-Linguistic Variation and Efficiency}  Oxford University Press.
\newline
\newline
Hobbs, Jerry R. and Grishman, Ralph (1975) ``The Automatic Transformational Analysis of English Sentences: An Implementation''  {\it International Journal of Computer Mathematics}
5:1-4: 267-283.
\newline
\newline
P Hofmeister, TF Jaeger, I Arnon, IA Sag, N Snider (2013) ``The Source Ambiguity Problem: Distinguishing the Effects of Grammar and Processing on Acceptability Judgments''
{\it Language and Cognitive Processes} 28 (1-2): 48-87
\newline
\newline
Hofmeister, Philip, and Ivan A. Sag. 2010. ``Cognitive Constraints and Island Effects''
{\it Language}
86(2): 366–415.
\newline
\newline
Hsaio, Franny and Edward Gibson (2003) ``Processing relative clauses in Chinese'' {\it Cognition} 90: 3-27.
\newline
\newline
Kayne, Richard S. (1994) {\it The Antisymmetry of Syntax}.  Cambridge, MA:  MIT Press.
\newline
\newline
Keenan, Edward L. and Bernard Comrie (1977) ``Noun phrase accessibility and universal grammar''. {\it Linguistic Inquiry} 8: 63-99.
\newline
\newline
King, Jonathan and Marcel Just  (1991) ``Individual differences in syntactic processing: the role of working memory'' {\it Journal of Memory and Language} 30:580–602.
\newline
\newline
Kluender, Robert and Marta Kutas (1993) ``Bridging the Gap: Evidence from ERPs on the Processing of Unbounded Dependencies'' {\it Journal of Cognitive Neuroscience} 5(2):196-214.
\newline
\newline
Konieczny, Lars (1996) {\it Human Sentence Processing:
A Semantics-Oriented Parsing Approach} unpublished doctoral dissertation, University of Freiburg.
\newline
\newline
Lin, Chien-Jer Charles  and Thomas Bever (2006) ``Subject Preference in the Processing of Relative Clauses in Chinese'' D. Baumer, D. Montero, M. Scanlon(eds) {\it Proceedings of the 25th West Coast Conference on Formal Linguistics}: 254–260. Cascadilla Proceedings Project.
\newline
\newline
Linadarki, Evita (2006) {\it Linguistic and Statistical Extensions of Data
Oriented Parsing} unpublished doctoral dissertation, University of Essex
\newline
\newline
MacDonald, Maryellen C., 
Neal J. Pearlmutter,
and Mark S. Seidenberg  (1994).  ``The Lexical Nature of Syntactic 
Ambiguity Resolution'' 
{\it Psychological Review} 101: 676-703
\newline
\newline
Marslen-Wilson, William, and Lorraine Tyler (1987) ``Against Modularity'', in J. L.
Garfield (ed) {\it Modularity in Knowledge Representation and Natural Language Processing}
MIT Press
\newline
\newline
McGurk, Harry and John MacDonald (1976). ``Hearing Lips and Seeing Voices'' {\it Nature} 264 (5588): 746–748
\newline
\newline
McMurray, Bob, Meghan A. Clayards, Michael K. Tanenhaus, and Richard Aslin  (2008) ``Tracking the Time Course of Phonetic Cue Integration During Spoken Word Recognition''. {\it Psychonomic Bulletin and Review} 15: 1064
\newline
\newline
Miyamoto, Edson T. and
Michiko Nakamura (2003) ``Subject/Object
Asymmetries
in the Processing
of Relative Clauses
in Japanese''.  G. Garding and M. Tsujimura (eds) {\it Proceedings of the 22nd Meeting of the West Coast Conference on Formal Linguistics}:  342-55. Somerville, MA: 
Cascadilla Press. 
\newline
\newline
Miyao, Yusuke  and Junichi Tsujii (2008) ``Feature Forest Models for
Probabilistic HPSG Parsing  {\it Computational Linguistics} 34(1): 35-80
\newline
\newline
Phillips, Colin (2013) ``On the Nature of Island Constraints I: Language Processing and Reductionist Accounts'' in
J. Sprouse and N. Hornstein (eds.).
{\it Experimental Syntax
and Island Effects}: 64-108.
Cambridge University
Press.
\newline
\newline
Pollard, Carl and Ivan A. Sag (1994) {\it Head-Driven Phrase Structure Grammar}  CSLI Publications.
\newline
\newline
Ross, John R. (1967) {\it Constraints on Variables in Syntax}, doctoral dissertation, MIT  (Published in revised form in 1986 under the title {\it Infinite Syntax!} by Norwood Publishing)
\newline
\newline
Sag, Ivan (1997) ``English Relative Clause Constructions''.  {\it Journal of
Linguistics}
33:  
431-483.
\newline
\newline
Sag, Ivan and Thomas Wasow (2011) ``Performance-Compatible Competence Grammar'' in R. Borsley and K. Börjars (eds) {\it Non-Transformational Syntax: Formal and Explicit Models of Grammar}: 359-377. Wiley-Blackwell
\newline
\newline
Sag, Ivan and Thomas Wasow (2015) ``Flexible Processing and the Design of Grammar'' {\it Journal of Psycholinguistic Research} 44: 47-63
\newline
\newline
Sag, Ivan A. Thomas Wasow, and Emily Bender (2003) {\it Syntactic Theory: A Formal Introduction (Second Edition)} CSLI Publications
\newline
\newline
Spevack, Samuel C., 
J. Benjamin Falandays,
Brandon Batzloff, and
Michael J. Spivey (2018) ``Interactivity of Language'' {\it Language and Linguistics Compass} 12
\newline
\newline
Sprouse, Jon, Matt Wagers and Colin Phillips (2012) ``A Test of the Relation Between Working-Memory Capacity and Syntactic Island Effects''.  {\it Language}  Vol. 88(1): 82-123
\newline
\newline
Stroop, John Ridley (1935) ``Studies of interference in serial verbal reactions'' {\it Journal of Experimental Psychology} 18(6): 643–662
\newline
\newline
Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C. Sedivy
``Integration of Visual and Linguistic Information in Spoken Language Comprehension''  (1995)
{\it Science}, 268(5217): 1632-1634
\newline
\newline
Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C. Sedivy (1996)  ``Using Eye Movements to Study Spoken Language Comprehension:  Evidence for Visually Mediated Incremental Interpretation''  in T. Inui and J.L. McClelland (eds), {\it Attention and Performance XVI:  Information Integration in Perception and Communication}: 457-478. MIT Press. 
\newline
\newline
Tanenhaus, Michael K. and Trueswell, John C. (1995)  ``Sentence comprehension'' in J. Miller and P. Eimas (eds) {\it Handbook of Perception and Cognition}, Vol. 11.  Academic Press.
\newline
\newline
Traxler, Matthew J., Robin K. Morris, and Rachel E. Seely (2002) ``Processing Subject and Object Relative Clauses: 
Evidence from Eye Movements'' {\it Journal of Memory and Language} 47: 69–90 
\newline
\newline
Traxler, Matthew J. and Kristen M.Tooley (2007) ``Lexical Mediation and Context Effects in Sentence Processing''  {\it Brain Research}
1146: 59-74
\newline
\newline
Trueswell, John C., Michael K. Tanenhaus, and Christopher Kello (1993) ``Verb-specific Constraints in Sentence Processing: Separating Effects of Lexical Preference from Garden-Paths'' {\it Journal of Experimental Psychology: Learning, Memory, and Cognition} 19(3): 528-553
\newline
\newline
Vasishth, Shravan, Chen Zhong, Qiang Li, and Gueilan Guo (2013) ``Processing Chinese Relative Clauses: Evidence for the Subject-Relative Advantage'' {\it PLoS ONE} 8(10): e77006.
\newline    
\newline
Wanner, Eric and Michael Maratsos (1978) ``An ATN approach to comprehension'', in M. Halle, J. Bresnan, and G.A. Miller (Eds.), {\it Linguistic Theory and Psychological Reality}: 119-161. MIT Press
\newline
\newline
Wasow, Thomas  (2015) ``Ambiguity Avoidance is Overrated'' in Susanne Winkler (ed) {\it Ambiguity: Language and Communication}, 29-47. DeGruyter
 \newline
\newline
Wasow, Thomas, T. Florian Jaeger, and David Orr (2011) ``Lexical Variation in Relativizer Frequency'' in H. Simon and H. Wiese (eds) {\it Expecting the Unexpected: Exceptions in Grammar}: 175-195. De Gruyter 
\end{document}
